{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940d26c3",
   "metadata": {},
   "source": [
    "## VQE ansatz: 001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42055090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import AdamOptimizer, GradientDescentOptimizer\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from wordsToNumbers import Corpus\n",
    "from wordsToNumbers import fibonacci_vocabulary\n",
    "\n",
    "from wordsToQubits import put_word_on_sphere\n",
    "\n",
    "from utils import get_corpus_from_directory, working_window, get_word_from_sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25724fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a727b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4951bf91",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1bc2987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr. words:1648 \n",
      "nr. distinct words: 499 \n",
      "len.text/len.vocab:3.302605210420842\n"
     ]
    }
   ],
   "source": [
    "corpus_path='/Users/voicutu/Desktop/Qountry/CountryMixt'\n",
    "\n",
    "corpus_tex = get_corpus_from_directory(corpus_path, limit=1)\n",
    "\n",
    "corpus= Corpus(corpus_tex)\n",
    "print(corpus.prop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a79b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterize_vovabulary = fibonacci_vocabulary(corpus.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3761a237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: ['same', 'old', 'dive', 'same', 'old', 'end', 'of', 'the', 'work', 'week', 'drink', 'bartender', 'knows', 'my', 'name', 'but', 'i', 'dont', 'mind', 'she', 'kicks', 'em', 'up', 'strong', 'serves', 'me', 'up', 'right', 'and', 'here', 'i', 'go', 'again', 'im', 'drinkin', 'one', 'im', 'drinkin', 'two', 'i', 'got', 'my', 'heartache', 'medication', 'a', 'strong', 'dedication', 'to', 'gettin', 'over', 'you', 'turnin', 'me', 'loose', 'on', 'that', 'hardwood', 'jukebox', 'lost', 'in', 'neon', 'time', 'my', 'heartache', 'medication', 'well', 'it', 'suits', 'me', 'fine', 'and', 'im', 'drinkin', 'enough', 'to', 'take', 'you', 'off', 'my', 'mind', 'i', 'got', 'my', 'heartache', 'medication', 'well', 'in', 'my', 'time', 'of', 'dying', 'dont', 'want', 'nobody', 'to', 'mourn', 'all', 'i', 'want', 'for', 'you', 'to', 'do', 'is', 'take', 'my', 'body', 'home', 'well', 'well', 'well', 'so', 'i', 'can', 'die', 'easy', 'well', 'well', 'well', 'well', 'well', 'well', 'so', 'i', 'can', 'die', 'easy', 'jesus', 'gonna', 'make', 'up', 'jesus', 'gonna', 'make', 'up', 'jesus', 'gonna', 'make', 'up', 'my', 'dying', 'bed', 'well', 'meet', 'me', 'jesus', 'meet', 'me', 'meet', 'me', 'in', 'the', 'middle', 'of', 'the', 'air', 'if', 'these', 'wings', 'should', 'fail', 'to', 'me', 'lord', 'wont', 'you', 'meet', 'me', 'with', 'another', 'pair', 'well', 'well', 'well', 'so', 'i', 'can', 'die', 'easy', 'well', 'well', 'well', 'well', 'well', 'well', 'so', 'i', 'can', 'die', 'easy', 'jesus', 'gonna', 'make', 'up', 'jesus', 'gonna', 'make', 'up', 'jesus', 'gonna', 'make', 'up', 'my', 'dying', 'bed', 'lord', 'in', 'my', 'time', 'of', 'dying', 'dont', 'want', 'nobody', 'to', 'cry', 'all', 'i', 'want', 'you', 'to', 'do', 'is', 'take', 'me', 'when', 'i', 'die', 'well', 'well', 'well', 'so', 'i', 'can', 'die', 'easy', 'well', 'well', 'well', 'well', 'well', 'well', 'so', 'i', 'can', 'die', 'easy', 'jesus', 'gonna', 'make', 'up', 'jesus', 'gonna', 'make', 'up', 'jesus', 'gonna', 'make', 'up', 'my', 'dying', 'bed', 'put', 'on', 'my', 'jeans', 'my', 'favorite', 'shirt', 'pull', 'up', 'my', 'boots', 'and', 'hit', 'the', 'dirt', 'finally', 'doin', 'somethin', 'ive', 'dreamed', 'of', 'for', 'years', 'dont', 'know', 'quite', 'what', 'to', 'expect', 'a', 'little', 'scared', 'but', 'what', 'the', 'heck', 'my', 'desire', 'is', 'always', 'greater', 'than', 'my', 'fear', 'big', 'dreams', 'and', 'faded', 'jeans', 'fit', 'together', 'like', 'a', 'team', 'always', 'busting', 'at', 'the', 'seams', 'big', 'dreams', 'and', 'faded', 'jeans', 'just', 'my', 'ole', 'guitar', 'and', 'me', 'out', 'to', 'find', 'my', 'destiny', 'nashville', 'is', 'the', 'place', 'to', 'be', 'for', 'big', 'dreams', 'and', 'faded', 'jeans', 'put', 'out', 'my', 'thumb', 'and', 'wish', 'for', 'luck', 'to', 'hitch', 'a', 'car', 'a', 'semitruck', 'sooner', 'or', 'later', 'one', 'will', 'catch', 'me', 'in', 'their', 'beams', 'then', 'ill', 'be', 'on', 'my', 'way', 'at', 'last', 'find', 'a', 'future', 'lose', 'a', 'past', 'waiting', 'silent', 'as', 'the', 'passion', 'in', 'me', 'screams', 'big', 'dreams', 'and', 'faded', 'jeans', 'fit', 'together', 'like', 'a', 'team', 'always', 'busting', 'at', 'the', 'seams', 'big', 'dreams', 'and', 'faded', 'jeans', 'may', 'the', 'stars', 'that', 'fill', 'my', 'eyes', 'guide', 'my', 'path', 'and', 'be', 'my', 'light', 'and', 'may', 'god', 'provide', 'the', 'means', 'to', 'accomplish', 'my', 'big', 'dreams', 'my', 'big', 'dreams', 'big', 'dreams', 'and', 'faded', 'jeans', 'fit', 'together', 'like', 'a', 'team', 'always', 'busting', 'at', 'the', 'seams', 'big', 'dreams', 'and', 'faded', 'jeans', 'like', 'the', 'song', 'bobby', 'mcgee', 'freedoms', 'just', 'another', 'word', 'im', 'just', 'longin', 'to', 'be', 'free', 'to', 'be', 'free', 'take', 'me', 'where', 'i', 'want', 'to', 'be', 'big', 'dreams', 'and', 'faded', 'jeans', 'and', 'there', 'are', 'many', 'just', 'like', 'me', 'with', 'big', 'dreams', 'and', 'faded', 'jeans', 'mmhmmmmmm', 'mmhmmmmmm', 'big', 'dreams', 'big', 'dreams', 'and', 'faded', 'jeans', 'ohohohoh', 'ohohohoh', 'im', 'just', 'longing', 'to', 'be', 'free', 'take', 'me', 'where', 'i', 'want', 'to', 'be', 'big', 'dreams', 'and', 'faded', 'jeans', 'well', 'i', 'dont', 'know', 'why', 'i', 'love', 'you', 'like', 'i', 'do', 'nobody', 'in', 'the', 'world', 'can', 'get', 'along', 'with', 'you', 'you', 'got', 'the', 'ways', 'of', 'a', 'devil', 'sleeping', 'in', 'a', 'lions', 'den', 'i', 'come', 'home', 'last', 'night', 'you', 'wouldnt', 'even', 'let', 'me', 'in', 'well', 'sometimes', 'youre', 'as', 'sweet', 'as', 'anybody', 'want', 'to', 'be', 'when', 'you', 'get', 'a', 'crazy', 'notion', 'of', 'jumpin', 'all', 'over', 'me', 'well', 'you', 'give', 'me', 'the', 'blues', 'i', 'guess', 'youre', 'satisfied', 'an', 'you', 'give', 'me', 'the', 'blues', 'i', 'wanna', 'lay', 'down', 'and', 'die', 'i', 'helped', 'you', 'when', 'you', 'had', 'no', 'shoes', 'on', 'your', 'feet', 'pretty', 'mama', 'i', 'helped', 'you', 'when', 'you', 'had', 'no', 'food', 'to', 'eat', 'youre', 'the', 'kind', 'of', 'woman', 'i', 'just', 'dont', 'understand', 'youre', 'takin', 'all', 'my', 'money', 'and', 'give', 'it', 'to', 'another', 'man', 'well', 'youre', 'the', 'kinda', 'woman', 'makes', 'a', 'man', 'lose', 'his', 'brain', 'youre', 'the', 'kinda', 'woman', 'drives', 'a', 'man', 'insane', 'you', 'give', 'me', 'the', 'blues', 'i', 'guess', 'youre', 'satisfied', 'you', 'give', 'me', 'the', 'blues', 'i', 'wanna', 'lay', 'down', 'and', 'die', 'well', 'you', 'give', 'me', 'the', 'blues', 'i', 'wanna', 'lay', 'down', 'and', 'die', 'i', 'am', 'a', 'man', 'of', 'constant', 'sorrow', 'ive', 'seen', 'trouble', 'all', 'my', 'days', 'ill', 'say', 'goodbye', 'to', 'colorado', 'where', 'i', 'was', 'born', 'and', 'partly', 'raised', 'your', 'mother', 'says', 'that', 'im', 'a', 'stranger', 'a', 'face', 'youll', 'never', 'see', 'no', 'more', 'but', 'heres', 'one', 'promise', 'to', 'ya', 'ill', 'see', 'you', 'on', 'gods', 'golden', 'shore', 'through', 'this', 'open', 'world', 'im', 'abound', 'to', 'ramble', 'through', 'ice', 'and', 'snow', 'sleet', 'and', 'rain', 'im', 'abound', 'to', 'ride', 'that', 'mornin', 'railroad', 'perhaps', 'ill', 'die', 'upon', 'that', 'train', 'im', 'agoin', 'back', 'to', 'colorado', 'the', 'place', 'that', 'ive', 'started', 'from', 'if', 'id', 'knowed', 'how', 'bad', 'youd', 'treat', 'me', 'babe', 'i', 'never', 'would', 'have', 'come', 'ive', 'been', 'around', 'this', 'whole', 'country', 'but', 'i', 'never', 'yet', 'found', 'fenneario', 'well', 'as', 'we', 'marched', 'down', 'as', 'we', 'marched', 'down', 'well', 'as', 'we', 'marched', 'down', 'to', 'fennerio', 'well', 'our', 'captain', 'fell', 'in', 'love', 'with', 'a', 'lady', 'like', 'a', 'dove', 'her', 'name', 'that', 'she', 'had', 'was', 'pretty', 'peggyo', 'well', 'what', 'will', 'your', 'mother', 'say', 'what', 'will', 'your', 'mother', 'say', 'what', 'will', 'your', 'mother', 'say', 'pretty', 'peggyo', 'what', 'will', 'your', 'mother', 'say', 'to', 'know', 'youre', 'going', 'away', 'youre', 'never', 'never', 'never', 'coming', 'backio', 'come', 'arunning', 'down', 'your', 'stairs', 'come', 'arunning', 'down', 'your', 'stairs', 'come', 'arunning', 'down', 'your', 'stairs', 'pretty', 'peggyo', 'come', 'arunning', 'down', 'your', 'stairs', 'combing', 'back', 'your', 'yellow', 'hair', 'youre', 'the', 'prettiest', 'darned', 'girl', 'i', 'ever', 'seenio', 'the', 'lieutenant', 'he', 'has', 'gone', 'the', 'lieutenant', 'he', 'has', 'gone', 'the', 'lieutenant', 'he', 'has', 'gone', 'pretty', 'peggyo', 'the', 'lieutenant', 'he', 'has', 'gone', 'long', 'gone', 'hes', 'ariding', 'down', 'in', 'texas', 'with', 'the', 'rodeo', 'well', 'our', 'captain', 'he', 'is', 'dead', 'our', 'captain', 'he', 'is', 'dead', 'our', 'captain', 'he', 'is', 'dead', 'pretty', 'peggyo', 'well', 'our', 'captain', 'he', 'is', 'dead', 'died', 'for', 'a', 'maid', 'hes', 'buried', 'somewhere', 'in', 'louisianao', 'feeling', 'funny', 'in', 'my', 'mind', 'lord', 'i', 'believe', 'im', 'fixing', 'to', 'die', 'fixing', 'to', 'die', 'feeling', 'funny', 'in', 'my', 'mind', 'lord', 'i', 'believe', 'im', 'fixing', 'to', 'die', 'well', 'i', 'dont', 'mind', 'dying', 'but', 'i', 'hate', 'to', 'leave', 'my', 'children', 'crying', 'well', 'i', 'look', 'over', 'yonder', 'to', 'that', 'burying', 'ground', 'look', 'over', 'yonder', 'to', 'that', 'burying', 'ground', 'sure', 'seems', 'lonesome', 'lord', 'when', 'the', 'sun', 'goes', 'down', 'feeling', 'funny', 'in', 'my', 'eyes', 'lord', 'i', 'believe', 'im', 'fixing', 'to', 'die', 'fixing', 'to', 'die', 'feeling', 'funny', 'in', 'my', 'eyes', 'lord', 'i', 'believe', 'im', 'fixing', 'to', 'die', 'well', 'i', 'dont', 'mind', 'dying', 'but', 'i', 'hate', 'to', 'leave', 'my', 'children', 'crying', 'theres', 'a', 'black', 'smoke', 'rising', 'lord', 'its', 'rising', 'up', 'above', 'my', 'head', 'up', 'above', 'my', 'head', 'its', 'rising', 'up', 'above', 'my', 'head', 'up', 'above', 'my', 'head', 'and', 'tell', 'jesus', 'make', 'up', 'my', 'dying', 'bed', 'im', 'walking', 'kind', 'of', 'funny', 'lord', 'i', 'believe', 'im', 'fixing', 'to', 'die', 'fixing', 'to', 'die', 'yes', 'im', 'walking', 'kind', 'of', 'funny', 'lord', 'i', 'believe', 'im', 'fixing', 'to', 'die', 'fixing', 'to', 'die', 'fixing', 'to', 'die', 'well', 'i', 'dont', 'mind', 'dying', 'but', 'i', 'hate', 'to', 'leave', 'my', 'children', 'crying', 'ramblin', 'outa', 'the', 'wild', 'west', 'leavin', 'the', 'towns', 'i', 'love', 'the', 'best', 'thought', 'id', 'seen', 'some', 'ups', 'and', 'downs', 'til', 'i', 'come', 'into', 'new', 'york', 'town', 'people', 'goin', 'down', 'to', 'the', 'ground', 'buildings', 'goin', 'up', 'to', 'the', 'sky', 'wintertime', 'in', 'new', 'york', 'town', 'the', 'wind', 'blowin', 'snow', 'around', 'walk', 'around', 'with', 'nowhere', 'to', 'go', 'somebody', 'could', 'freeze', 'right', 'to', 'the', 'bone', 'i', 'froze', 'right', 'to', 'the', 'bone', 'new', 'york', 'times', 'said', 'it', 'was', 'the', 'coldest', 'winter', 'in', 'seventeen', 'years', 'i', 'didnt', 'feel', 'so', 'cold', 'then', 'i', 'swung', 'onto', 'my', 'old', 'guitar', 'grabbed', 'hold', 'of', 'a', 'subway', 'car', 'and', 'after', 'a', 'rocking', 'reeling', 'rolling', 'ride', 'i', 'landed', 'up', 'on', 'the', 'downtown', 'side', 'greenwich', 'village', 'i', 'walked', 'down', 'there', 'and', 'ended', 'up', 'in', 'one', 'of', 'them', 'coffeehouses', 'on', 'the', 'block', 'got', 'on', 'the', 'stage', 'to', 'sing', 'and', 'play', 'man', 'there', 'said', 'come', 'back', 'some', 'other', 'day', 'you', 'sound', 'like', 'a', 'hillbilly', 'we', 'want', 'folk', 'singers', 'here', 'well', 'i', 'got', 'a', 'harmonica', 'job', 'begun', 'to', 'play', 'blowin', 'my', 'lungs', 'out', 'for', 'a', 'dollar', 'a', 'day', 'i', 'blowed', 'inside', 'out', 'and', 'upside', 'down', 'the', 'man', 'there', 'said', 'he', 'loved', 'm', 'sound', 'he', 'was', 'ravin', 'about', 'how', 'he', 'loved', 'm', 'sound', 'dollar', 'a', 'days', 'worth', 'and', 'after', 'weeks', 'and', 'weeks', 'of', 'hangin', 'around', 'i', 'finally', 'got', 'a', 'job', 'in', 'new', 'york', 'town', 'in', 'a', 'bigger', 'place', 'bigger', 'money', 'too', 'even', 'joined', 'the', 'union', 'and', 'paid', 'm', 'dues', 'now', 'a', 'very', 'great', 'man', 'once', 'said', 'that', 'some', 'people', 'rob', 'you', 'with', 'a', 'fountain', 'pen', 'it', 'didnt', 'take', 'too', 'long', 'to', 'find', 'out', 'just', 'what', 'he', 'was', 'talkin', 'about', 'a', 'lot', 'of', 'people', 'dont', 'have', 'much', 'food', 'on', 'their', 'table', 'but', 'they', 'got', 'a', 'lot', 'of', 'forks', 'n', 'knives', 'and', 'they', 'gotta', 'cut', 'somethin', 'so', 'one', 'mornin', 'when', 'the', 'sun', 'was', 'warm', 'i', 'rambled', 'out', 'of', 'new', 'york', 'town', 'pulled', 'my', 'cap', 'down', 'over', 'my', 'eyes', 'and', 'headed', 'out', 'for', 'the', 'western', 'skies', 'so', 'long', 'new', 'york', 'howdy', 'east', 'orange', 'i', 'first', 'heard', 'this', 'from', 'ric', 'von', 'schmidt', 'he', 'lives', 'in', 'cambridge', 'ric', 'is', 'a', 'blues', 'guitar', 'player', 'i', 'met', 'him', 'one', 'day', 'on', 'the', 'green', 'pastures', 'of', 'the', 'harvard', 'university', 'baby', 'let', 'me', 'follow', 'you', 'down', 'baby', 'let', 'me', 'follow', 'you', 'down', 'well', 'ill', 'do', 'anything', 'in', 'this', 'god', 'almighty', 'world', 'if', 'you', 'just', 'let', 'me', 'follow', 'you', 'down', 'can', 'i', 'come', 'home', 'with', 'you', 'baby', 'can', 'i', 'come', 'home', 'with', 'you', 'yes', 'ill', 'do', 'anything', 'in', 'this', 'god', 'almighty', 'world', 'if', 'you', 'just', 'let', 'me', 'come', 'home', 'with', 'you', 'baby', 'let', 'me', 'follow', 'you', 'down', 'baby', 'let', 'me', 'follow', 'you', 'down', 'well', 'ill', 'do', 'anything', 'in', 'this', 'god', 'almighty', 'world', 'if', 'you', 'just', 'let', 'me', 'follow', 'you', 'down', 'yes', 'ill', 'do', 'anything', 'in', 'this', 'god', 'almighty', 'world', 'if', 'you', 'just', 'let', 'me', 'follow', 'you', 'down']\n"
     ]
    }
   ],
   "source": [
    "print(\"corpus:\",corpus.split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc7ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23056c1a",
   "metadata": {},
   "source": [
    "## Training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "484b6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lenghth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c733f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = working_window(history_lenghth, splited_text=corpus.split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0689e8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len training set: 1644\n"
     ]
    }
   ],
   "source": [
    "print(\"len training set:\", len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275e6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46257ec7",
   "metadata": {},
   "source": [
    "## Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58423cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20aced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=history_lenghth+1)\n",
    "\n",
    "# circuit initializer\n",
    "def circuit_initializer(words):\n",
    "    for i in range(len(words)):\n",
    "        put_word_on_sphere(words[i], qubit=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c34fdfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_type1(param, wires=[0,1,2]):\n",
    "\n",
    "    qml.CRZ( param[0], wires=[0,1])\n",
    "    qml.CRX( param[1], wires=[0,1])\n",
    "    qml.CRY( param[2], wires=[0,1])\n",
    "\n",
    "    qml.CRZ( param[3], wires=[1,2])\n",
    "    qml.CRX( param[4], wires=[1,2])\n",
    "    qml.CRY( param[5], wires=[1,2])\n",
    "\n",
    "    qml.CRZ( param[6], wires=[2,3])\n",
    "    qml.CRX( param[7], wires=[2,3])\n",
    "    qml.CRY( param[8], wires=[2,3])\n",
    "\n",
    "    qml.RX( param[9], wires=0)\n",
    "    qml.RY( param[10], wires=0)\n",
    "\n",
    "    qml.RX( param[11], wires=1)\n",
    "    qml.RY( param[12], wires=1)\n",
    "\n",
    "    qml.RX( param[13], wires=2)\n",
    "    qml.RY( param[14], wires=2)\n",
    "\n",
    "    qml.RX( param[15], wires=3)\n",
    "    qml.RY( param[16], wires=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a97158",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def next_gen(params, x, obs='z'):\n",
    "    \"\"\"\n",
    "    obs:'z', 'x' or 'y'\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the circuit \n",
    "    circuit_initializer(x)\n",
    "\n",
    "    # \n",
    "    for param in params:\n",
    "        layer_type1(param, wires=[0,1,2])\n",
    "    #circuit_initializer(x) # just for a test \n",
    "\n",
    "\n",
    "    # measure \n",
    "    if obs=='z':\n",
    "        return  qml.expval(qml.PauliZ(3))\n",
    "    if obs=='x':\n",
    "        return qml.expval(qml.PauliX(3))\n",
    "    if obs=='y':\n",
    "        return qml.expval(qml.PauliY(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2217ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: ['same', 'old', 'dive']\n",
      "x_vec: [[-0.5174781860030842, -0.6546184738955823, 0.5510816460793957], [-0.19127602086694068, 0.26104417670682734, 0.9461867794726179], [-0.3628495838337743, 0.9317269076305221, 0.015004969474187188]]\n"
     ]
    }
   ],
   "source": [
    "x_vec=[ parameterize_vovabulary[w] for w in x[0]]\n",
    "print(\"x:\",x[0])\n",
    "print(\"x_vec:\",x_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35e93b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: ['same', 'old', 'dive']\n",
      "x_vec: [[-0.5174781860030842, -0.6546184738955823, 0.5510816460793957], [-0.19127602086694068, 0.26104417670682734, 0.9461867794726179], [-0.3628495838337743, 0.9317269076305221, 0.015004969474187188]]\n",
      "example tensor: [[-0.5174781860030842, -0.6546184738955823, 0.5510816460793957], [-0.19127602086694068, 0.26104417670682734, 0.9461867794726179], [-0.3628495838337743, 0.9317269076305221, 0.015004969474187188]]\n",
      "\n",
      "\n",
      "\n",
      " 0: ──RY(5.3)───RZ(0.902)───╭C──────────╭C──────────╭C───────────RX(0.311)───RY(0.404)────────────────────────────────────────────────────────────────────────┤     \n",
      " 1: ──RY(5.95)──RZ(-0.938)──╰RZ(0.643)──╰RX(0.539)──╰RY(0.512)──╭C──────────╭C──────────╭C───────────RX(0.597)───RY(0.241)────────────────────────────────────┤     \n",
      " 2: ──RY(4.73)──RZ(-1.2)────────────────────────────────────────╰RZ(0.61)───╰RX(0.493)──╰RY(0.214)──╭C──────────╭C──────────╭C──────────RX(0.906)──RY(0.159)──┤     \n",
      " 3: ────────────────────────────────────────────────────────────────────────────────────────────────╰RZ(0.244)──╰RX(0.572)──╰RY(0.773)──RX(0.589)──RY(0.262)──┤ ⟨Z⟩ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_vec=[ parameterize_vovabulary[w] for w in x[0]]\n",
    "print(\"x:\",x[0])\n",
    "print(\"x_vec:\",x_vec)\n",
    "\n",
    "params= np.random.uniform(size=(1, 17), requires_grad=True)\n",
    "\n",
    "print(\"example tensor:\", x_vec )\n",
    "print(\"\\n\\n\")\n",
    "print(qml.draw(next_gen)(params,x_vec,obs='z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59d15992",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vector=[ next_gen(params,x_vec, obs=o) for o in ['x', 'y', 'z']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b16a4",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d76f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_target_distance(pred_vector, target):\n",
    "\n",
    "    distance= np.linalg.norm(pred_vector-target)\n",
    "\n",
    "    #np.sqrt((pred_vector[0]-target[0])*(pred_vector[0]-target[0])+\n",
    "    #                 (pred_vector[1]-target[1])*(pred_vector[1]-target[1])+\n",
    "    #                 (pred_vector[2]-target[2])*(pred_vector[2]-target[2]))\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87594d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target word: same\n",
      "target vector: [-0.5174781860030842, -0.6546184738955823, 0.5510816460793957]\n",
      "prediction: [tensor(0.41123432, requires_grad=True), tensor(-0.66660704, requires_grad=True), tensor(0.43325996, requires_grad=True)]\n",
      "prediction word: then\n",
      "distance: 0.9362331935842173\n"
     ]
    }
   ],
   "source": [
    "print(\"target word:\",y[0])\n",
    "print(\"target vector:\",parameterize_vovabulary[y[0]] )\n",
    "print(\"prediction:\",pred_vector)\n",
    "pred_word= get_word_from_sphere(pred_vector, parameterize_vovabulary)\n",
    "print(\"prediction word:\",pred_word)\n",
    "distance= pred_target_distance(np.array(pred_vector), np.array(parameterize_vovabulary[y[0]]))\n",
    "print(\"distance:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb981b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance check 0.0\n"
     ]
    }
   ],
   "source": [
    "distance= pred_target_distance( np.array(parameterize_vovabulary[y[1]]), np.array(parameterize_vovabulary[y[1]]))\n",
    "print(\"distance check\",distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abb9c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost( par , x, y):\n",
    "\n",
    "    predictions = [[next_gen(par, w_input,obs='x') ,next_gen(par,w_input,obs='y') ,next_gen(par,w_input,obs='z') ] for w_input in x ]\n",
    "\n",
    "    c=0.0\n",
    "    for i in range(len(predictions)):\n",
    "        c = c+ pred_target_distance(np.array(predictions[i]), y)\n",
    "\n",
    "    c=c/len(predictions)\n",
    "\n",
    "    return np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18dc8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, y):\n",
    "    pred_words=[ get_word_from_sphere(p_v, parameterize_vovabulary) for p_v in predictions]\n",
    "    target_words=[ get_word_from_sphere(p_v, parameterize_vovabulary) for p_v in y]\n",
    "    ac=0\n",
    "    for i in range(len(pred_words)):\n",
    "        if pred_words[i]==target_words[i]:\n",
    "            ac=ac+1\n",
    "    return ac/len(pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bba265",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b071ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 1641 1642 1643]\n"
     ]
    }
   ],
   "source": [
    "## shuffling data \n",
    "index = np.array([ i for i in range(len(x))])\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb08af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= []\n",
    "Y_train= []\n",
    "Y_train_w= []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    vec = [parameterize_vovabulary[w] for w in x[i]]\n",
    "    X_train.append(vec)\n",
    "    Y_train.append(parameterize_vovabulary[y[i]])\n",
    "    Y_train_w.append(y[i])\n",
    "    \n",
    "X_train= np.array(X_train)\n",
    "Y_train= np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddfa12e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(X,Y, batch_size):\n",
    "    X1 = [torch.reshape(x[0], (1, 2 ** (len(spec.latent_qubits) + len(spec.trash_qubits)))) for x in X]\n",
    "    X2 = []\n",
    "    for i in range(len(X1)):\n",
    "        X2.append([X1[1], X[i][1]])\n",
    "    X = X2\n",
    "    random.shuffle(X)\n",
    "\n",
    "    batch_list = []\n",
    "    batch = []\n",
    "    for x in X:\n",
    "        if len(batch) < batch_size:\n",
    "            batch.append(x)\n",
    "\n",
    "        else:\n",
    "            batch_list.append(batch)\n",
    "            batch = []\n",
    "    if len(batch) != 0:\n",
    "        batch_list.append(batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d587df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model parameters \n",
    "\n",
    "num_layers= 1\n",
    "layer_param= 17\n",
    "params = np.random.uniform(size=(2, layer_param), requires_grad=True)\n",
    "\n",
    "learning_rate= 0.6\n",
    "opt = AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999)\n",
    "\n",
    "nr_epochs= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42d331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 | train_cost:53.60082886916693\n",
      "ac: 0.0036496350364963502\n",
      "Iter:1 | train_cost:51.22300344453689\n",
      "Iter:2 | train_cost:43.94168400175275\n",
      "Iter:3 | train_cost:43.02241166220944\n",
      "Iter:4 | train_cost:42.20706998707202\n",
      "Iter:5 | train_cost:42.22409169057897\n",
      "Iter:6 | train_cost:41.569128379960745\n",
      "Iter:7 | train_cost:41.62247286223286\n",
      "Iter:8 | train_cost:41.42697478304665\n",
      "Iter:9 | train_cost:41.5571161180561\n",
      "Iter:10 | train_cost:41.756217205647744\n",
      "ac: 0.0006082725060827251\n",
      "Iter:11 | train_cost:41.81138947351213\n",
      "Iter:12 | train_cost:41.50899039197157\n",
      "Iter:13 | train_cost:41.284097932138984\n",
      "Iter:14 | train_cost:41.23511784896323\n",
      "Iter:15 | train_cost:41.445282858198446\n",
      "Iter:16 | train_cost:41.17631706803689\n",
      "Iter:17 | train_cost:41.183101421089425\n",
      "Iter:18 | train_cost:41.23648509725077\n",
      "Iter:19 | train_cost:41.10445623032547\n",
      "Iter:20 | train_cost:41.017359535920136\n",
      "ac: 0.0024330900243309003\n",
      "Iter:21 | train_cost:41.03232950789743\n",
      "Iter:22 | train_cost:40.9921434744548\n",
      "Iter:23 | train_cost:40.91168859941484\n",
      "Iter:24 | train_cost:40.90038964487262\n",
      "Iter:25 | train_cost:40.952299071754375\n",
      "Iter:26 | train_cost:40.988422094601354\n",
      "Iter:27 | train_cost:40.95422328514514\n",
      "Iter:28 | train_cost:40.90829257270129\n",
      "Iter:29 | train_cost:40.90018313118352\n",
      "Iter:30 | train_cost:40.88306680664558\n",
      "ac: 0.0012165450121654502\n",
      "Iter:31 | train_cost:40.83698749880915\n",
      "Iter:32 | train_cost:40.81772024588079\n",
      "Iter:33 | train_cost:40.842393586182894\n",
      "Iter:34 | train_cost:40.847978358903966\n",
      "Iter:35 | train_cost:40.85586675149081\n",
      "Iter:36 | train_cost:40.84505101082819\n",
      "Iter:37 | train_cost:40.82948022774587\n",
      "Iter:38 | train_cost:40.83122520838564\n",
      "Iter:39 | train_cost:40.83144270608448\n",
      "Iter:40 | train_cost:40.82484053590407\n",
      "ac: 0.0018248175182481751\n",
      "Iter:41 | train_cost:40.813587633435525\n",
      "Iter:42 | train_cost:40.817518854074315\n",
      "Iter:43 | train_cost:40.82084855168148\n",
      "Iter:44 | train_cost:40.80781319113658\n",
      "Iter:45 | train_cost:40.800607371763206\n",
      "Iter:46 | train_cost:40.79900676198739\n",
      "Iter:47 | train_cost:40.79845083193922\n",
      "Iter:48 | train_cost:40.7974971332933\n",
      "Iter:49 | train_cost:40.78847005479553\n",
      "Iter:50 | train_cost:40.79113997930021\n",
      "ac: 0.0012165450121654502\n",
      "Iter:51 | train_cost:40.78958247560156\n",
      "Iter:52 | train_cost:40.78662617999686\n",
      "Iter:53 | train_cost:40.786079406342964\n",
      "Iter:54 | train_cost:40.785515100734465\n",
      "Iter:55 | train_cost:40.784763711690886\n",
      "Iter:56 | train_cost:40.78106252834827\n",
      "Iter:57 | train_cost:40.77781541291851\n",
      "Iter:58 | train_cost:40.7764720168225\n",
      "Iter:59 | train_cost:40.778424404169556\n",
      "Iter:60 | train_cost:40.77639905838941\n",
      "ac: 0.0012165450121654502\n",
      "Iter:61 | train_cost:40.77476293072475\n",
      "Iter:62 | train_cost:40.77450443020996\n",
      "Iter:63 | train_cost:40.77332795258603\n",
      "Iter:64 | train_cost:40.77115516313853\n",
      "Iter:65 | train_cost:40.76895670954605\n"
     ]
    }
   ],
   "source": [
    "ls_progres=[]\n",
    "ac_progres=[]\n",
    "for e in range(nr_epochs):\n",
    "    params, ls = opt.step_and_cost(lambda p: cost( par=p, x=X_train, y=Y_train),params)\n",
    "\n",
    "    print(\"Iter:{} | train_cost:{}\".format(e, ls))\n",
    "    ls_progres.append(ls)\n",
    "\n",
    "\n",
    "    if e%10==0:\n",
    "        predictions = [[next_gen(params, w_input,obs='x') ,next_gen(params,w_input,obs='y') ,next_gen(params,w_input,obs='z') ] for w_input in X_train ]\n",
    "        ac=accuracy(predictions, y=Y_train)\n",
    "\n",
    "        print(\"ac:\",ac)\n",
    "        ac_progres.append(ac)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9fa59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa2be45b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22685a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(ls_progres))],np.array(ls_progres),label=\"train loss\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"loss_VQE_001_C1\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "print(\"last loss:\",ls_progres[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7972e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in range(0,len(ac_progres)*10,10)],np.array(ac_progres),label=\"train accuracy\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"accuracy_VQE_001_C1\",)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accyracy\")\n",
    "\n",
    "print(\"accuracy loss:\",ac_progres[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d84294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    \n",
    "    words_vec = [parameterize_vovabulary[w] for w in x[i]]\n",
    "    pred_vector=[ next_gen(params,words_vec, obs=o) for o in ['x', 'y', 'z']]\n",
    "    pred_word= get_word_from_sphere(pred_vector, parameterize_vovabulary)\n",
    "    \n",
    "    text=\"\"\n",
    "    for w in x[i];\n",
    "    text=text+w+\" \"\n",
    "    \n",
    "    print(\"{} {}|{}\".format(text, pred_word,y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5e5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
