{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11cb1798",
   "metadata": {},
   "source": [
    "## Encoder generator: 002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3161b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import AdamOptimizer, GradientDescentOptimizer\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from wordsToNumbers import Corpus\n",
    "from wordsToNumbers import fibonacci_vocabulary\n",
    "\n",
    "from wordsToQubits import put_word_on_sphere\n",
    "\n",
    "from utils import get_corpus_from_directory, working_window, get_word_from_sphere\n",
    "\n",
    "from qencode.initialize import setAux\n",
    "from qencode.encoders import e2_classic\n",
    "from qencode.training_circuits import swap_t\n",
    "from qencode.qubits_arrangement import QubitsArrangement\n",
    "\n",
    "from training.cost.swap_cost import sw_cost, sw_fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b556ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd51b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f3aadf1",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4290db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr. words:15 \n",
      "nr. distinct words: 13 \n",
      "len.text/len.vocab:1.1538461538461537\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "corpus_path=\"C:/Users/tomut/Documents/GitHub/Qountry/CountryMixt/\"\n",
    "\n",
    "corpus_tex = get_corpus_from_directory(corpus_path, limit=1)\n",
    "\n",
    "corpus= Corpus(corpus_tex)\n",
    "print(corpus.prop())\n",
    "\"\"\"\n",
    "corpus_text = \"Same old dive, same old end of the work week drink Bartender knows my name,\"#but I don't mind She kicks 'em up strong, serves me up right And here I go again I'm drinkin' one, I'm drinkin' two I got my heartache medication, a strong dedication To gettin' over you, turnin' me loose On that hardwood jukebox lost in neon time My heartache medication, well it suits me fine And I'm drinkin' enough to take you off my mind I got my heartache medication\"\n",
    "corpus= Corpus(corpus_text)\n",
    "print(corpus.prop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a040d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterize_vovabulary = fibonacci_vocabulary(corpus.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1498593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d34d04cb",
   "metadata": {},
   "source": [
    "## Training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222d368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lenghth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1052698",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = working_window(history_lenghth, splited_text=corpus.split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44485929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len training set: 11\n"
     ]
    }
   ],
   "source": [
    "print(\"len training set:\", len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172ae3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e26919e",
   "metadata": {},
   "source": [
    "## Working principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62ad28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ba401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e18aa1d",
   "metadata": {},
   "source": [
    "## Just a simple encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb194a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6064de7",
   "metadata": {},
   "source": [
    "### BIG encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17dcdac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qubits: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "shots = 2500\n",
    "nr_trash=2\n",
    "nr_latent=2\n",
    "\n",
    "spec_big = QubitsArrangement(nr_trash, nr_latent, nr_swap=1, nr_ent=0)\n",
    "print(\"Qubits:\", spec_big.qubits)\n",
    "\n",
    "#set up the device \n",
    "dev = qml.device(\"default.qubit\", wires=spec_big.num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdef6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameterize_vovabulary = fibonacci_vocabulary(corpus.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc53f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# circuit initializer\n",
    "def circuit_initializer(words, qubits):\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        print(\"i words\",words)\n",
    "        put_word_on_sphere(words[i], qubit=qubits[i])\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def encoder_e2(init_params, encoder_params,spec=spec_big, reinit_state=None):\n",
    "    #initilaization\n",
    "    circuit_initializer(init_params,qubits= [*spec.latent_qubits, *spec.trash_qubits])\n",
    "   \n",
    "    \n",
    "    #encoder\n",
    "    for params in encoder_params:\n",
    "        e2_classic(params, [*spec.latent_qubits, *spec_big.trash_qubits])\n",
    "\n",
    "    #swap test \n",
    "    swap_t(spec)\n",
    "\n",
    "    return [qml.probs(i) for i in spec.swap_qubits]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6cc8c",
   "metadata": {},
   "source": [
    "#### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12369000",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "batch_size = 2\n",
    "num_samples = 0.8 # proportion of the data used for training \n",
    "\n",
    "learning_rate = 0.0003\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "opt = AdamOptimizer(learning_rate, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55e07a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data example: tensor([[-0.8005, -0.5000,  0.3304],\n",
      "        [ 0.0000,  1.0000,  0.0000],\n",
      "        [-0.4545, -0.1667, -0.8750],\n",
      "        [-0.8005, -0.5000,  0.3304]])\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "\n",
    "for i in range(int(len(x)*num_samples)):\n",
    "    w_l = [ parameterize_vovabulary[w] for w in x[i]]\n",
    "    w_l.append(parameterize_vovabulary[y[i]])\n",
    "    training_data.append(w_l)\n",
    "training_data = torch.tensor(training_data)\n",
    "\n",
    "test_data = [ ]\n",
    "for i in range(int(len(x)*num_samples),len(x)):\n",
    "    w_l=[parameterize_vovabulary[w] for w in x[i]]\n",
    "    w_l.append(parameterize_vovabulary[y[i]])\n",
    "    test_data.append(w_l)\n",
    "test_data = torch.tensor(test_data)\n",
    "\n",
    "print(\"data example:\",training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef53141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(X, batch_size):\n",
    "   \n",
    "    X1 = [x  for x in X]\n",
    "    \n",
    "    \n",
    "    random.shuffle(X1)\n",
    "\n",
    "    batch_list = []\n",
    "    batch = []\n",
    "    for x in X:\n",
    "        if len(batch) < batch_size:\n",
    "            batch.append(x)\n",
    "\n",
    "        else:\n",
    "            batch_list.append(batch)\n",
    "            batch = []\n",
    "    if len(batch) != 0:\n",
    "        batch_list.append(batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6944a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[-0.8005, -0.5000,  0.3304],\n",
       "          [ 0.0000,  1.0000,  0.0000],\n",
       "          [-0.4545, -0.1667, -0.8750],\n",
       "          [-0.8005, -0.5000,  0.3304]]),\n",
       "  tensor([[ 0.0000,  1.0000,  0.0000],\n",
       "          [-0.4545, -0.1667, -0.8750],\n",
       "          [-0.8005, -0.5000,  0.3304],\n",
       "          [ 0.0000,  1.0000,  0.0000]])],\n",
       " [tensor([[-0.8005, -0.5000,  0.3304],\n",
       "          [ 0.0000,  1.0000,  0.0000],\n",
       "          [-0.0000, -1.0000, -0.0000],\n",
       "          [-0.9284,  0.3333, -0.1642]]),\n",
       "  tensor([[ 0.0000,  1.0000,  0.0000],\n",
       "          [-0.0000, -1.0000, -0.0000],\n",
       "          [-0.9284,  0.3333, -0.1642],\n",
       "          [ 0.8856, -0.3333,  0.3234]])],\n",
       " [tensor([[-0.9284,  0.3333, -0.1642],\n",
       "          [ 0.8856, -0.3333,  0.3234],\n",
       "          [-0.4076,  0.8333,  0.3734],\n",
       "          [ 0.5269,  0.5000,  0.6873]]),\n",
       "  tensor([[ 0.8856, -0.3333,  0.3234],\n",
       "          [-0.4076,  0.8333,  0.3734],\n",
       "          [ 0.5269,  0.5000,  0.6873],\n",
       "          [ 0.3159, -0.6667, -0.6751]])]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_list=iterate_batches(X=training_data, batch_size=2)\n",
    "batch_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3172f",
   "metadata": {},
   "source": [
    "###  training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce5ba275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize random encoder parameters\n",
    "nr_encod_qubits = len(spec_big.trash_qubits) + len(spec_big.latent_qubits)\n",
    "nr_par_encoder =  15 * int(nr_encod_qubits*(nr_encod_qubits-1)/2)\n",
    "encoder_params = np.random.uniform(size=(1, nr_par_encoder), requires_grad=True)\n",
    "\n",
    "#print(qml.draw(encoder_e2)(init_params=training_data[0], encoder_params=encoder_params, spec=spec_big))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2162eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomut\\anaconda3\\envs\\qountry\\lib\\site-packages\\pennylane\\math\\multi_dispatch.py:65: UserWarning: Contains tensors of types {'autograd', 'torch'}; dispatch will prioritize TensorFlow and PyTorch over autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     encoder_params \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mstep(cost, encoder_params, X\u001b[38;5;241m=\u001b[39mxbatch)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 23\u001b[0m     loss_training \u001b[38;5;241m=\u001b[39m \u001b[43mcost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     fidel \u001b[38;5;241m=\u001b[39m fidelity(encoder_params, training_data )\n\u001b[0;32m     26\u001b[0m     loss_hist\u001b[38;5;241m.\u001b[39mappend(loss_training)\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mcost\u001b[1;34m(encoder_params, X)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost\u001b[39m(encoder_params, X):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msw_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_e2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinit_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Qountry\\working_notebook\\..\\training\\cost\\swap_cost.py:23\u001b[0m, in \u001b[0;36msw_cost\u001b[1;34m(encoder_params, input_data, circuit, reinit_state)\u001b[0m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m input_data:\n\u001b[1;32m---> 23\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcircuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinit_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreinit_state\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     24\u001b[0m     f \u001b[38;5;241m=\u001b[39m fid_func(output)\n\u001b[0;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m f\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qountry\\lib\\site-packages\\pennylane\\qnode.py:550\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    547\u001b[0m         set_shots(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_device, override_shots)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_gradient_fn)()\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# construct the tape\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    553\u001b[0m using_custom_cache \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__setitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__delitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    557\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\qountry\\lib\\site-packages\\pennylane\\qnode.py:488\u001b[0m, in \u001b[0;36mQNode.construct\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mJacobianTape()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qfunc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    490\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mget_parameters(trainable_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mtrainable_params \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mget_trainable_indices(params)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mencoder_e2\u001b[1;34m(init_params, encoder_params, spec, reinit_state)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;129m@qml\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(dev)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder_e2\u001b[39m(init_params, encoder_params,spec\u001b[38;5;241m=\u001b[39mspec_big, reinit_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#initilaization\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mcircuit_initializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43mqubits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_qubits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrash_qubits\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#encoder\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m encoder_params:\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mcircuit_initializer\u001b[1;34m(words, qubits)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcircuit_initializer\u001b[39m(words, qubits):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words)):\n\u001b[1;32m----> 4\u001b[0m         \u001b[43mput_word_on_sphere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqubit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqubits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Qountry\\working_notebook\\..\\wordsToQubits\\encoding\\word_to_polar.py:17\u001b[0m, in \u001b[0;36mput_word_on_sphere\u001b[1;34m(params, qubit)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mput_word_on_sphere\u001b[39m(params, qubit):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    :param params: [x, y ,z ]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    :param qubit:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m:\n\u001b[0;32m     18\u001b[0m         theta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marctan(np\u001b[38;5;241m.\u001b[39msqrt(params[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m params[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m params[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m params[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m params[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "def cost(encoder_params, X):\n",
    "    return sw_cost(encoder_params, input_data=X, circuit=encoder_e2, reinit_state=None)\n",
    "\n",
    "def fidelity(encoder_params, X):\n",
    "    print(\"x\",X)\n",
    "    return sw_fidelity(encoder_params, input_data=X, circuit=encoder_e2, reinit_state=None)\n",
    "  \n",
    "    \n",
    "loss_hist=[]\n",
    "fid_hist=[]\n",
    "\n",
    "loss_hist_test=[]\n",
    "fid_hist_test=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batches = iterate_batches(X=training_data, batch_size=batch_size)\n",
    "    for xbatch in batches:\n",
    "        encoder_params = opt.step(cost, encoder_params, X=xbatch)\n",
    "\n",
    "        \n",
    "    if epoch%5 == 0:\n",
    "        \n",
    "        loss_training = cost(encoder_params, [training_data] )\n",
    "        fidel = fidelity(encoder_params, training_data )\n",
    "        \n",
    "        loss_hist.append(loss_training)\n",
    "        fid_hist.append(fidel)\n",
    "        print(\"Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_training, fidel))\n",
    "\n",
    "        loss_test = cost(encoder_params, test_data )\n",
    "        fidel = fidelity(encoder_params, test_data )\n",
    "        loss_hist_test.append(loss_test)\n",
    "        fid_hist_test.append(fidel)\n",
    "        print(\"Test-Epoch:{} | Loss:{} | Fidelity:{}\".format(epoch, loss_test, fidel))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        experiment_parameters={\"experiment\":\"Encoder_002\",\"params\":encoder_params}\n",
    "        f=open(\"Encoder_002_/params\"+str(epoch)+\".txt\",\"w\")\n",
    "        f.write(str(experiment_parameters))\n",
    "        f.close()\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5bb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce7a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69198d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
